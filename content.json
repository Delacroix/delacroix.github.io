{"meta":{"title":"Delacroix's Blog","subtitle":null,"description":"Ŭ����Ϊһ��CCIE","author":"Delacroix","url":"https://delacroix.github.io"},"pages":[],"posts":[{"title":"","slug":"ELK完成操作系统日志审计","date":"2017-11-24T02:47:01.868Z","updated":"2017-11-24T02:47:01.868Z","comments":true,"path":"2017/11/24/ELK完成操作系统日志审计/","link":"","permalink":"https://delacroix.github.io/2017/11/24/ELK完成操作系统日志审计/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Kubernetes系统架构简介","slug":"Kubernetes系统架构简介","date":"2017-02-13T03:12:30.000Z","updated":"2017-02-13T03:17:54.059Z","comments":true,"path":"2017/02/13/Kubernetes系统架构简介/","link":"","permalink":"https://delacroix.github.io/2017/02/13/Kubernetes系统架构简介/","excerpt":"","text":"【转】Kubernetes系统架构简介原文 http://www.infoq.com/cn/articles/Kubernetes-system-architecture-introduction 1. 前言 Together we will ensure that Kubernetes is a strong and open container management framework for any application and in any environment, whether in a private, public or hybrid cloud. Urs Hölzle, Google Kubernetes作为Docker生态圈中重要一员，是Google多年大规模容器管理技术的开源版本，是产线实践经验的最佳表现[G1] 。如Urs Hölzle所说，无论是公有云还是私有云甚至混合云，Kubernetes将作为一个为任何应用，任何环境的容器管理框架无处不在。正因为如此， 目前受到各大巨头及初创公司的青睐，如Microsoft、VMWare、Red Hat、CoreOS、Mesos等，纷纷加入给Kubernetes贡献代码。随着Kubernetes社区及各大厂商的不断改进、发展，Kuberentes将成为容器管理领域的领导者。 接下来我们会用一系列文章逐一探索Kubernetes是什么、能做什么以及怎么做。 2. 什么是KubernetesKubernetes是Google开源的容器集群管理系统，其提供应用部署、维护、 扩展机制等功能，利用Kubernetes能方便地管理跨机器运行容器化的应用，其主要功能如下： 1) 使用Docker对应用程序包装(package)、实例化(instantiate)、运行(run)。 2) 以集群的方式运行、管理跨机器的容器。 3) 解决Docker跨机器容器之间的通讯问题。 4) Kubernetes的自我修复机制使得容器集群总是运行在用户期望的状态。 当前Kubernetes支持GCE、vShpere、CoreOS、OpenShift、Azure等平台，除此之外，也可以直接运行在物理机上。 接下来本文主要从以下几方面阐述Kubernetes： 1) Kubernetes的主要概念。 2) Kubernetes的构件，包括Master组件、Kubelet、Proxy的详细介绍。 3. Kubernetes主要概念3.1. PodsPod是Kubernetes的基本操作单元，把相关的一个或多个容器构成一个Pod，通常Pod里的容器运行相同的应用。Pod包含的容器运行在同一个Minion(Host)上，看作一个统一管理单元，共享相同的volumes和network namespace/IP和Port空间。 3.2. ServicesServices也是Kubernetes的基本操作单元，是真实应用服务的抽象，每一个服务后面都有很多对应的容器来支持，通过Proxy的port和服务selector决定服务请求传递给后端提供服务的容器，对外表现为一个单一访问接口，外部不需要了解后端如何运行，这给扩展或维护后端带来很大的好处。 3.3. Replication ControllersReplication Controller确保任何时候Kubernetes集群中有指定数量的pod副本(replicas)在运行， 如果少于指定数量的pod副本(replicas)，Replication Controller会启动新的Container，反之会杀死多余的以保证数量不变。Replication Controller使用预先定义的pod模板创建pods，一旦创建成功，pod 模板和创建的pods没有任何关联，可以修改pod 模板而不会对已创建pods有任何影响，也可以直接更新通过Replication Controller创建的pods。对于利用pod 模板创建的pods，Replication Controller根据label selector来关联，通过修改pods的label可以删除对应的pods。Replication Controller主要有如下用法： 1) Rescheduling 如上所述，Replication Controller会确保Kubernetes集群中指定的pod副本(replicas)在运行， 即使在节点出错时。 2) Scaling 通过修改Replication Controller的副本(replicas)数量来水平扩展或者缩小运行的pods。 3) Rolling updates Replication Controller的设计原则使得可以一个一个地替换pods来rolling updates服务。 4) Multiple release tracks 如果需要在系统中运行multiple release的服务，Replication Controller使用labels来区分multiple release tracks。 3.4. LabelsLabels是用于区分Pod、Service、Replication Controller的key/value键值对，Pod、Service、 Replication Controller可以有多个label，但是每个label的key只能对应一个value。Labels是Service和Replication Controller运行的基础，为了将访问Service的请求转发给后端提供服务的多个容器，正是通过标识容器的labels来选择正确的容器。同样，Replication Controller也使用labels来管理通过pod 模板创建的一组容器，这样Replication Controller可以更加容易，方便地管理多个容器，无论有多少容器。 4. Kubernetes构件Kubenetes整体框架如下图3-1，主要包括kubecfg、Master API Server、Kubelet、Minion(Host)以及Proxy。图3-1 Kubernetes High Level构件 4.1. MasterMaster定义了Kubernetes 集群Master/API Server的主要声明，包括Pod Registry、Controller Registry、Service Registry、Endpoint Registry、Minion Registry、Binding Registry、RESTStorage以及Client, 是client(Kubecfg)调用Kubernetes API，管理Kubernetes主要构件Pods、Services、Minions、容器的入口。Master由API Server、Scheduler以及Registry等组成。从下图3-2可知Master的工作流主要分以下步骤： 1) Kubecfg将特定的请求，比如创建Pod，发送给Kubernetes Client。 2) Kubernetes Client将请求发送给API server。 3) API Server根据请求的类型，比如创建Pod时storage类型是pods，然后依此选择何种REST Storage API对请求作出处理。 4) REST Storage API对的请求作相应的处理。 5) 将处理的结果存入高可用键值存储系统Etcd中。 6) 在API Server响应Kubecfg的请求后，Scheduler会根据Kubernetes Client获取集群中运行Pod及Minion信息。 7) 依据从Kubernetes Client获取的信息，Scheduler将未分发的Pod分发到可用的Minion节点上。 下面是Master的主要构件的详细介绍：图3-2 Master主要构件及工作流 3.1.1. Minion RegistryMinion Registry负责跟踪Kubernetes 集群中有多少Minion(Host)。Kubernetes封装Minion Registry成实现Kubernetes API Server的RESTful API接口REST，通过这些API，我们可以对Minion Registry做Create、Get、List、Delete操作，由于Minon只能被创建或删除，所以不支持Update操作，并把Minion的相关配置信息存储到etcd。除此之外，Scheduler算法根据Minion的资源容量来确定是否将新建Pod分发到该Minion节点。 3.1.2. Pod RegistryPod Registry负责跟踪Kubernetes集群中有多少Pod在运行，以及这些Pod跟Minion是如何的映射关系。将Pod Registry和Cloud Provider信息及其他相关信息封装成实现Kubernetes API Server的RESTful API接口REST。通过这些API，我们可以对Pod进行Create、Get、List、Update、Delete操作，并将Pod的信息存储到etcd中，而且可以通过Watch接口监视Pod的变化情况，比如一个Pod被新建、删除或者更新。 3.1.3. Service RegistryService Registry负责跟踪Kubernetes集群中运行的所有服务。根据提供的Cloud Provider及Minion Registry信息把Service Registry封装成实现Kubernetes API Server需要的RESTful API接口REST。利用这些接口，我们可以对Service进行Create、Get、List、Update、Delete操作，以及监视Service变化情况的watch操作，并把Service信息存储到etcd。 3.1.4. Controller RegistryController Registry负责跟踪Kubernetes集群中所有的Replication Controller，Replication Controller维护着指定数量的pod 副本(replicas)拷贝，如果其中的一个容器死掉，Replication Controller会自动启动一个新的容器，如果死掉的容器恢复，其会杀死多出的容器以保证指定的拷贝不变。通过封装Controller Registry为实现Kubernetes API Server的RESTful API接口REST， 利用这些接口，我们可以对Replication Controller进行Create、Get、List、Update、Delete操作，以及监视Replication Controller变化情况的watch操作，并把Replication Controller信息存储到etcd。 3.1.5. Endpoints RegistryEndpoints Registry负责收集Service的endpoint(注：服务暴露的接口)，比如Name：”mysql”，Endpoints: [“10.10.1.1:1909”，”10.10.2.2:8834”]，同Pod Registry，Controller Registry也实现了Kubernetes API Server的RESTful API接口，可以做Create、Get、List、Update、Delete以及watch操作。 3.1.6. Binding RegistryBinding包括一个需要绑定Pod的ID和Pod被绑定的Host，Scheduler写Binding Registry后，需绑定的Pod被绑定到一个host。Binding Registry也实现了Kubernetes API Server的RESTful API接口，但Binding Registry是一个write-only对象，所有只有Create操作可以使用， 否则会引起错误。 3.1.7. SchedulerScheduler收集和分析当前Kubernetes集群中所有Minion节点的资源(内存、CPU)负载情况，然后依此分发新建的Pod到Kubernetes集群中可用的节点。由于一旦Minion节点的资源被分配给Pod，那这些资源就不能再分配给其他Pod， 除非这些Pod被删除或者退出， 因此，Kubernetes需要分析集群中所有Minion的资源使用情况，保证分发的工作负载不会超出当前该Minion节点的可用资源范围。具体来说，Scheduler做以下工作： 1) 实时监测Kubernetes集群中未分发的Pod。 2) 实时监测Kubernetes集群中所有运行的Pod，Scheduler需要根据这些Pod的资源状况安全地将未分发的Pod分发到指定的Minion节点上。 3) Scheduler也监测Minion节点信息，由于会频繁查找Minion节点，Scheduler会缓存一份最新的信息在本地。 4) 最后，Scheduler在分发Pod到指定的Minion节点后，会把Pod相关的信息Binding写回API Server。 4.2. Kubelet图3-3 Kubernetes详细构件 根据上图3-3可知Kubelet是Kubernetes集群中每个Minion和Master API Server的连接点，Kubelet运行在每个Minion上，是Master API Server和Minion之间的桥梁，接收Master API Server分配给它的commands和work，与持久性键值存储etcd、file、server和http进行交互，读取配置信息。Kubelet的主要工作是管理Pod和容器的生命周期，其包括Docker Client、Root Directory、Pod Workers、Etcd Client、Cadvisor Client以及Health Checker组件，具体工作如下： 1) 通过Worker给Pod异步运行特定的Action。 2) 设置容器的环境变量。 3) 给容器绑定Volume。 4) 给容器绑定Port。 5) 根据指定的Pod运行一个单一容器。 6) 杀死容器。 7) 给指定的Pod创建network 容器。 8) 删除Pod的所有容器。 9) 同步Pod的状态。 10) 从Cadvisor获取container info、 pod info、root info、machine info。 11) 检测Pod的容器健康状态信息。 12) 在容器中运行命令。 4.3. ProxyProxy是为了解决外部网络能够访问跨机器集群中容器提供的应用服务而设计的，从上图3-3可知Proxy服务也运行在每个Minion上。Proxy提供TCP/UDP sockets的proxy，每创建一种Service，Proxy主要从etcd获取Services和Endpoints的配置信息，或者也可以从file获取，然后根据配置信息在Minion上启动一个Proxy的进程并监听相应的服务端口，当外部请求发生时，Proxy会根据Load Balancer将请求分发到后端正确的容器处理。","categories":[],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://delacroix.github.io/tags/Kubernetes/"},{"name":"架构","slug":"架构","permalink":"https://delacroix.github.io/tags/架构/"},{"name":"Docker","slug":"Docker","permalink":"https://delacroix.github.io/tags/Docker/"}]},{"title":"mysqldump_error_1066_not_unique_table","slug":"mysqldump-error-1066-not-unique-table","date":"2017-02-09T07:01:54.000Z","updated":"2017-02-09T07:07:34.172Z","comments":true,"path":"2017/02/09/mysqldump-error-1066-not-unique-table/","link":"","permalink":"https://delacroix.github.io/2017/02/09/mysqldump-error-1066-not-unique-table/","excerpt":"","text":"1mysqldump: Got error: 1066: Not unique table/alias myql 导出时提示如下： 12[root@localhost MySQL]# mysqldump -uroot -p 123456 test &gt;test_bakmysqldump: Got error: 1066: Not unique table/alias: 'robots_excludeurl' when using LOCK TABLES 修改/etc/my.cnf，将下面这行用#注释掉即可：lower_case_table_names=1（等于1表示不区分表名大小写）注释掉后，重启mysql： 1service mysql restart 再导出，好了。","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://delacroix.github.io/tags/MySQL/"}]},{"title":"Umask简介","slug":"Umask简介","date":"2017-01-05T02:43:33.000Z","updated":"2017-01-05T03:07:13.044Z","comments":true,"path":"2017/01/05/Umask简介/","link":"","permalink":"https://delacroix.github.io/2017/01/05/Umask简介/","excerpt":"","text":"【转】Umask简介原文链接 https://linux.cn/article-3065-1.htmlUmask 是什么？解释下，UMASK 代表用户掩码或用户文件创建掩码，它用于新创建的文件和文件夹，是其默认权限的基础。上面的意思可以解释成任何基于 Linux 的操作系统为新建文件(包括文件夹)添加默认权限的规则。出于教育目的，下面列出了一些可以用来设置文件权限的八进制值掩码： 123456780 – 读, 写, 可执行 (rwx)1 – 读和写 (rw-)2 – 读和可执行 (r-x)3 – 只读 (r--)4 – 写和可执行 (-wx)5 – 只写 (-w-)6 – 仅可执行 (--x)7 – 没有权限 (---) 在几乎所有的 Linux 发行版本中默认 Umask 值是 0022(或022)，可以在终端模拟程序中输入 umask 命令来查看。也可以运行 “umask 八进制值掩码” 命令（例如 umask 027）来临时改变这个值。 你也许知道，新创建的文件的默认权限设置原本应该是 0666，文件夹的是 0777。应用上面所说的 umask 值后就得到 644 和 755 权限。 许多意见认为 022 掩码会带来隐私问题，也就是说您所创建的文件对其他用户来说是可随意查看的，一想到这就感到不太爽！ 言归正传，用户可以按他们所愿来修改默认的 Umask 值，当然首先要保证修改的值合法。要修改默认值，请在您的 shell 配置文件中或者 /etc/profile 文件中写入一个新的 Umask 值。 好了，这就大功告成了！从现在开始，在你的 Linux 系统上新创建的文件或文件夹都会有准确设置的权限。但请注意已经存在的文件或文件夹的权限并不会因为上面的操作而改变。 如果您使用的是命令行，可以在任意目录下运行 ls -lah 命令，就可以看到当前的文件权限。另外，新手也可以很容易地查看到文件权限，在基于 GNOME 桌面的环境中，通过右击文件，选择属性 -&gt; 权限选项卡。","categories":[{"name":"Linux","slug":"Linux","permalink":"https://delacroix.github.io/categories/Linux/"}],"tags":[{"name":"Operations","slug":"Operations","permalink":"https://delacroix.github.io/tags/Operations/"},{"name":"Operation System","slug":"Operation-System","permalink":"https://delacroix.github.io/tags/Operation-System/"},{"name":"Policy","slug":"Policy","permalink":"https://delacroix.github.io/tags/Policy/"}]},{"title":"Linux 接口ARP响应问题","slug":"Linux 接口ARP响应问题","date":"2016-12-22T06:48:33.000Z","updated":"2016-12-22T06:50:00.397Z","comments":true,"path":"2016/12/22/Linux 接口ARP响应问题/","link":"","permalink":"https://delacroix.github.io/2016/12/22/Linux 接口ARP响应问题/","excerpt":"","text":"Linux 接口ARP响应问题arp_announce/arp_ignore sysctlThe arp_announce/arp_ignore sysctl on interfaces is available at the Linux official kernel since 2.6.4 and 2.4.26. The description about arp_announce/arp_ignore taken from kernel documentation is as follows: arp_announce - INTEGERDefine different restriction levels for announcing the local source IP address from IP packets in ARP requests sent on interface: 0 - (default) Use any local address, configured on any interface 1 - Try to avoid local addresses that are not in the target’s subnet for this interface.This mode is useful when target hosts reachable via this interface require the source IP address in ARP requests to be part of their logical network configured on the receiving interface. When we generate the request we will check all our subnets that include the target IP and will preserve the source address if it is from such subnet. If there is no such subnet we select source address according to the rules for level 2. 2 - Always use the best local address for this target.In this mode we ignore the source address in the IP packet and try to select local address that we prefer for talks with the target host. Such local address is selected by looking for primary IP addresses on all our subnets on the outgoing interface that include the target IP address. If no suitable local address is found we select the first local address we have on the outgoing interface or on all other interfaces, with the hope we will receive reply for our request and even sometimes no matter the source IP address we announce. The max value from conf/{all,interface}/arp_announce is used. Increasing the restriction level gives more chance for receiving answer from the resolved target while decreasing the level announces more valid sender’s information. arp_ignore - INTEGERDefine different modes for sending replies in response to received ARP requests that resolve local target IP addresses:0 - (default): reply for any local target IP address, configured on any interface 1 - reply only if the target IP address is local address configured on the incoming interface 2 - reply only if the target IP address is local address configured on the incoming interface and both with the sender’s IP address are part from same subnet on this interface 3 - do not reply for local addresses configured with scope host, only resolutions for global and link addresses are replied 4-7 - reserved 8 - do not reply for all local addressesThe max value from conf/{all,interface}/arp_ignore is used when ARP request is received on the {interface} Disable ARP for VIPTo disable ARP for VIP at real servers, we just need to set arp_announce/arp_ignore sysctls at the interface connected to the VIP network.For example, real servers have eth0 connected to the VIP network with the VIP at interface lo, we will have the following commands. 12echo 1 &gt; /proc/sys/net/ipv4/conf/eth0/arp_ignoreecho 2 &gt; /proc/sys/net/ipv4/conf/eth0/arp_announce Or, if /etc/sysctl.conf is used in the system, we have this config in /etc/sysctl.conf 12net.ipv4.conf.eth0.arp_ignore = 1net.ipv4.conf.eth0.arp_announce = 2 If you’re about to brought up a new logical interface(eg. lo.0 etc.) it’s recommended to have the following commands. 12echo \"1\" &gt; /proc/sys/net/ipv4/conf/all/arp_ignoreecho \"2\" &gt; /proc/sys/net/ipv4/conf/all/arp_announce Note that the arp_announce/arp_ignore sysctls must be setup correctly, before the VIP address is brought up at a logical interface at real servers.","categories":[{"name":"Linux","slug":"Linux","permalink":"https://delacroix.github.io/categories/Linux/"}],"tags":[{"name":"Operations","slug":"Operations","permalink":"https://delacroix.github.io/tags/Operations/"},{"name":"Network","slug":"Network","permalink":"https://delacroix.github.io/tags/Network/"},{"name":"ARP","slug":"ARP","permalink":"https://delacroix.github.io/tags/ARP/"}]},{"title":"Jenkins + Ansible实现自动化发布","slug":"Jenkins + Ansible实现自动化发布","date":"2016-12-21T03:14:33.000Z","updated":"2016-12-21T03:35:44.936Z","comments":true,"path":"2016/12/21/Jenkins + Ansible实现自动化发布/","link":"","permalink":"https://delacroix.github.io/2016/12/21/Jenkins + Ansible实现自动化发布/","excerpt":"Jenkins + Ansible实现自动化发布前期在生产环境中进行应用发布时，采用纯粹手工的方式从测试环境拷贝WAR到生产环境目录进行备份、部署、重启服务等操作。为解决批量部署的效率问题，以及尽可能避免人工操作造成的不可预知的失误，准备采用自动化的方式来实现应用的发布。 本文档适用的前提：测试环境已经通过CI工具完成自动化编译、打包，并生成了路径、命名固定的WAR包，生产环境只需拷贝该WAR包到指定目录解压即可的场景。 Jenkins安装 部署环境：操作系统：ubuntu 14.04服务器IP：192.168.1.1 第一种方式前往Jenkins官网https://pkg.jenkins.io/debian-stable/，将官方Jenkins key引入。 1wget -q -O - https://pkg.jenkins.io/debian-stable/jenkins.io.key | sudo apt-key add - 然后将其加入源列表。 1deb https://pkg.jenkins.io/debian-stable binary/ 最后，更新本地包索引，并安装稳定版Jenkins 12sudo apt-get updatesudo apt-get install jenkins","text":"Jenkins + Ansible实现自动化发布前期在生产环境中进行应用发布时，采用纯粹手工的方式从测试环境拷贝WAR到生产环境目录进行备份、部署、重启服务等操作。为解决批量部署的效率问题，以及尽可能避免人工操作造成的不可预知的失误，准备采用自动化的方式来实现应用的发布。 本文档适用的前提：测试环境已经通过CI工具完成自动化编译、打包，并生成了路径、命名固定的WAR包，生产环境只需拷贝该WAR包到指定目录解压即可的场景。 Jenkins安装 部署环境：操作系统：ubuntu 14.04服务器IP：192.168.1.1 第一种方式前往Jenkins官网https://pkg.jenkins.io/debian-stable/，将官方Jenkins key引入。 1wget -q -O - https://pkg.jenkins.io/debian-stable/jenkins.io.key | sudo apt-key add - 然后将其加入源列表。 1deb https://pkg.jenkins.io/debian-stable binary/ 最后，更新本地包索引，并安装稳定版Jenkins 12sudo apt-get updatesudo apt-get install jenkins 第二种方式前往Jenkins官网https://pkg.jenkins.io/debian-stable/，下载deb包 1wget https://pkg.jenkins.io/debian-stable/binary/jenkins_2.19.4_all.deb 安装deb包 1dpkg -i jenkins_2.19.4_all.deb 按照操作提示步骤进行安装。完成后通过以下命令进行启动 1/etc/init.d/jenkins start 启动后，Jenkins默认将会监听8080端口。 安装Ansible过程略Ansible安装完成后，需要做以下工作： 实现被控服务器免密登陆在Ansible主控服务器的Jenkins用户下，执行以下命令生产密钥对： 1ssh-keygen -t rsa 将jenkins用户的公钥拷贝到被控服务器的ssh目录下： 12ssh-copy-id -i .ssh/id_rsa.pub app@192.168.1.2ssh-copy-id -i .ssh/id_rsa.pub app@172.16.1.2 至此，Ansible能够通过jenkins用户免密登陆应用服务器生产、测试环境。 Jenkins调用Ansible首先，需要在Jenkins上安装Ansible的插件，名为 Ansible plugin，在插件管理界面搜索安装即可。然后，在Jenkins新建Freestyle Project 调用的Ansible-playbook示例如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253- hosts: 192.168.1.2 vars: testIP: 172.16.1.2 testhome: /home/app/api-tomcat/webapps warname: api.war oldhome: /home/app/api-tomcat backupwebapps: /home/app/tomcat.bak newwar: /home/app/newwar zipname: api remote_user: app tasks: - name: 删除/home/app/newwar目录 file: path=&#123;&#123; newwar &#125;&#125; state=absent ignore_errors: yes - name: 创建/home/app/newwar目录.改权限 file: path=&#123;&#123; newwar &#125;&#125; recurse=yes mode=775 owner=app group=app state=directory - name: 从测试环境复制war包到/home/app/newwar目录 shell: scp app@&#123;&#123; testIP &#125;&#125;:&#123;&#123; testhome &#125;&#125;/&#123;&#123; warname &#125;&#125; &#123;&#123; newwar &#125;&#125; - name: 给/home/app/newwar递归改权限 file: dest=&#123;&#123; newwar&#125;&#125; recurse=yes mode=775 owner=app group=app - name: 解压/home/app/newwar目录内的war包在本目录内 shell: unzip -oq &#123;&#123; newwar &#125;&#125;/&#123;&#123; warname &#125;&#125; -d &#123;&#123; newwar &#125;&#125;/&#123;&#123; zipname &#125;&#125; - name: 再次给/home/app/newwar递归改权限 file: dest=&#123;&#123; newwar&#125;&#125; recurse=yes mode=775 owner=app group=app - name: 创建备份webapps目录/home/app/tomcat.bak并改权限 file: path=&#123;&#123; backupwebapps &#125;&#125;/&#123;&#123; zipname &#125;&#125; recurse=yes mode=775 owner=app group=app state=directory - name: 备份webapps到目录/home/app/tomcat.bak下并加上时间戳 shell: cp -a &#123;&#123; oldhome &#125;&#125;/webapps &#123;&#123; backupwebapps &#125;&#125;/&#123;&#123; zipname &#125;&#125;/webapps-`date +%Y%m%d%H%M` - name: kill进程方式停止服务.忽略错误返回值 shell: ps -ef | grep &#123;&#123; oldhome &#125;&#125; | grep -v grep | xargs kill ignore_errors: yes - name: kill进程方式停止服务.忽略错误返回值 shell: ps -ef | grep &#123;&#123; oldhome &#125;&#125; | grep -v grep | xargs kill ignore_errors: yes - name: 再次kill进程方式停止服务.忽略错误返回值 shell: ps -ef | grep &#123;&#123; oldhome &#125;&#125; | grep -v grep | xargs kill ignore_errors: yes - name: 查看停止服务的结果.进程是否还在 shell: ps -ef | grep &#123;&#123; oldhome &#125;&#125; - name: 删除原来的webapps目录下的war包 file: path=&#123;&#123; oldhome &#125;&#125;/webapps/&#123;&#123; warname &#125;&#125; state=absent ignore_errors: yes - name: 删除原来的webapps目录下的程序目录 file: path=&#123;&#123; oldhome &#125;&#125;/webapps/&#123;&#123; zipname &#125;&#125; state=absent ignore_errors: yes - name: 复制/home/app/newwar目录下的解压的新程序目录到webapps目录下 shell: cp -a &#123;&#123; newwar &#125;&#125;/&#123;&#123; zipname &#125;&#125; &#123;&#123; oldhome &#125;&#125;/webapps/ - name: 复制/home/app/newwar目录下的war包到webapps目录下 shell: cp -a &#123;&#123; newwar &#125;&#125;/&#123;&#123; warname &#125;&#125; &#123;&#123; oldhome &#125;&#125;/webapps/ - name: 启动服务 shell: \"source /etc/profile;nohup &#123;&#123; oldhome &#125;&#125;/bin/startup.sh &amp;\" - name: 查看进程中是否存在启动的服务 shell: ps -ef | grep &#123;&#123; oldhome &#125;&#125; 执行RUN，开始部署操作： 然后就可以在CONSOLE界面看到执行的结果：","categories":[{"name":"Linux","slug":"Linux","permalink":"https://delacroix.github.io/categories/Linux/"}],"tags":[{"name":"Operations","slug":"Operations","permalink":"https://delacroix.github.io/tags/Operations/"},{"name":"自动化","slug":"自动化","permalink":"https://delacroix.github.io/tags/自动化/"},{"name":"运维","slug":"运维","permalink":"https://delacroix.github.io/tags/运维/"}]},{"title":"8D 工作法","slug":"8D工作法","date":"2016-12-15T02:56:52.000Z","updated":"2016-12-15T07:09:41.408Z","comments":true,"path":"2016/12/15/8D工作法/","link":"","permalink":"https://delacroix.github.io/2016/12/15/8D工作法/","excerpt":"8D 工作法8D是解决问题的8条基本准则或称8个工作步骤，但在实际应用中却有9个步骤： D0：征兆紧急反应措施D1：小组成立D2：问题说明D3：实施并验证临时措施D4：确定并验证根本原因D5：选择和验证永久纠正措施D6：实施永久纠正措施D7：预防再发生D8：小组祝贺D0：征兆紧急反应措施","text":"8D 工作法8D是解决问题的8条基本准则或称8个工作步骤，但在实际应用中却有9个步骤： D0：征兆紧急反应措施D1：小组成立D2：问题说明D3：实施并验证临时措施D4：确定并验证根本原因D5：选择和验证永久纠正措施D6：实施永久纠正措施D7：预防再发生D8：小组祝贺D0：征兆紧急反应措施 目的:主要是为了看此类问题是否需要用8D来解决，如果问题太小，或是不适合用8D来解决的问题，例如价格，经费等等，这一步是针对问题发生时候的紧急反应。 关键要点: 判断问题的类型、大小、范畴等等。与D3不同，D0是针对问题发生的反应，而D3是针对产品或服务问题本身的暂时应对措施。 D1：小组成立 目的:成立一个小组,小组成员具备工艺/产品的知识,有配给的时间并授予了权限,同时应具有所要求的能解决问题和实施纠正措施的技术素质。小组必须有一个指导和小组长。 关键要点: 成员资格，具备工艺、产品的知识;目标 ;分工 ;程序 ;小组建设 D2：问题说明 目的:用量化的术语详细说明与该问题有关的内/外部顾客抱怨,如什么、地点、时间、程度、频率等。 “什么东西出了什么问题” 方法：质量风险评定，FMEA分析 关键要点:收集和组织所有有关数据以说明问题;问题说明是所描述问题的特别有用的数据的总结;审核现有数据，识别问题、确定范围;细分问题，将复杂问题细分为单个问题;问题定义，找到和顾客所确认问题一致的说明，”什么东西出了什么问题”，而原因又未知风险等级。 D3：实施并验证临时措施 目的:保证在永久纠正措施实施前，将问题与内外部顾客隔离。（原为唯一可选步骤，但发展至今都需采用） 方法：FMEA、DOE、PPM 关键要点: 评价紧急响应措施;找出和选择最佳”临时抑制措施”;决策 ;实施，并作好记录;验证（DOE、PPM分析、控制图等） D4：确定并验证根本原因 目的:用统计工具列出可以用来解释问题起因的所有潜在原因，将问题说明中提到的造成偏差的一系列事件或环境或原因相互隔离测试并确定产生问题的根本原因。 方法：FMEA、PPM、DOE、控制图、5why法 关键要点: 评估可能原因列表中的每一个原因;原因可否使问题排除;验证;控制计划 D5：选择并验证永久纠正措施 目的:在生产前测试方案，并对方案进行评审以确定所选的校正措施能够解决客户问题，同时对其它过程不会有不良影响。 方法：FMEA 关键要点: 重新审视小组成员资格;决策，选择最佳措施;重新评估临时措施，如必要重新选择;验证;管理层承诺执行永久纠正措施;控制计划 D6：实施永久纠正措施 目的:制定一个实施永久措施的计划，确定过程控制方法并纳入文件，以确保根本原因的消除。在生产中应用该措施时应监督其长期效果。 方法：防错、统计控制 关键要点: 重新审视小组成员;执行永久纠正措施，废除临时措施;利用故障的可测量性确认故障已经排除;控制计划、工艺文件修改 D7：预防再发生 目的:修改现有的管理系统、操作系统、工作惯例、设计与规程以防止这一问题与所有类似问题重复发生。 关键要点:选择预防措施;验证有效性;决策 ;组织、人员、设备、环境、材料、文件重新确定 D8：小组祝贺 目的:承认小组的集体努力，对小组工作进行总结并祝贺。 关键要点: 有选择的保留重要文档;流览小组工作，将心得形成文件;了解小组对解决问题的集体力量，及对解决问题作出的贡献;必要的物质、精神奖励。","categories":[{"name":"问题处理流程","slug":"问题处理流程","permalink":"https://delacroix.github.io/categories/问题处理流程/"}],"tags":[{"name":"ISO20000","slug":"ISO20000","permalink":"https://delacroix.github.io/tags/ISO20000/"}]},{"title":"OSPF的基本原理与实现","slug":"OSPF的基本原理与实现","date":"2016-11-04T03:14:33.000Z","updated":"2016-11-07T04:55:29.690Z","comments":true,"path":"2016/11/04/OSPF的基本原理与实现/","link":"","permalink":"https://delacroix.github.io/2016/11/04/OSPF的基本原理与实现/","excerpt":"OSPF的基本原理与实现OSPF（Open Shortest Path First，开放最短路径优先）是由IETF开发路径选择协议。OSPF是开放的链路状态路由协议。OSPFv2应用于IPv4中，由RFC2328规范。 OSPF的特征： 收敛快速； 支持支持大规模的网络； 使用Area的概念； Classless（无类）路由协议。支持VLSM、CIDR、不连续子网； 支持无大小限制、任意的度量值； 支持多条等价路径的负载匀衡； 使用保留的组播地址（224.0.0.5和224.0.0.6）； 支持更安全的路由选择认证； 使用可以跟踪外部路由的路由标记；","text":"OSPF的基本原理与实现OSPF（Open Shortest Path First，开放最短路径优先）是由IETF开发路径选择协议。OSPF是开放的链路状态路由协议。OSPFv2应用于IPv4中，由RFC2328规范。 OSPF的特征： 收敛快速； 支持支持大规模的网络； 使用Area的概念； Classless（无类）路由协议。支持VLSM、CIDR、不连续子网； 支持无大小限制、任意的度量值； 支持多条等价路径的负载匀衡； 使用保留的组播地址（224.0.0.5和224.0.0.6）； 支持更安全的路由选择认证； 使用可以跟踪外部路由的路由标记； OSPF基本的工作过程： Neighbor（邻居）宣告OSPF的路由器从所有启动0SPF协议的接口上发出Hello数据包。如果两台路由器共享一条公共数据链路,并且能够相互成功协商它们各自Hello数据包中所指定的某些参数,那么它们就成为了邻居(Neighbor)。 Adjacency（邻接）可以想象成为一条点到点的虚链路,它是在一些邻居路由器之间构成的。OSPF协议定义了一些网络类型和一些路由器类型的邻接关系。邻接关系的建立是由交换Hello信息的路由器类型和交换Hello信息的网络类型决定的。 LSA（Link State Advertisement，链路状态通告）每一台路由器都会在所有形成邻接关系的邻居之间发送链路状态通告(Link State Advertisement，链路状态通告)。LSA描述了路由器所有的链路、接口、路由器的邻居以及链路状态信息。这些链路可以是到一个末梢网络(stub network,是指没有和其他路由器相连的网络)的链路、到其他OSPF路由器的链路、到其他区域网络的链路,或是到外部网络(从其他的路由选择进程学习到的网络)的链路。由于这些链路状态信息的多样性,OSPF协议定义了许多LSA类型。 LSDB（Link State Database，链路状态数据库）每一台收到从邻居路由器发出的LSA的路由器都会把这些LSA记录在它的LSDB（Link State Database，链路状态数据库）当中,并且发送一份LSA的拷贝给该路由器的其他所有邻居。 Full（完全邻接）通过LSA泛洪扩散到整个区域,所有的路由器都会形成同样的链路状态数据库。 SPF（Short Path First，最短路径优先）当这些邻接的OSPF路由器的数据库完全相同时,每—台路由器都将以其自身为根,使用SPF算法来计算一个无环路的拓扑图,以描述它所知道的到达每一个目的地的最短路径(最小的路径代价)。这个拓扑图就是SPF算法树。 IP Routing Table（IP路由表）每一台路由器都将从SPF算法树中构建出自己的路由表。 /** RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS. LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*//var disqus_config = function () {this.page.url = PAGE_URL; // Replace PAGE_URL with your page’s canonical URL variablethis.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page’s unique identifier variable}; /(function() { // DON’T EDIT BELOW THIS LINEvar d = document, s = d.createElement(‘script’);s.src = ‘//delacroixsblog.disqus.com/embed.js’;s.setAttribute(‘data-timestamp’, +new Date());(d.head || d.body).appendChild(s);})();Please enable JavaScript to view the comments powered by Disqus.","categories":[{"name":"Network","slug":"Network","permalink":"https://delacroix.github.io/categories/Network/"}],"tags":[{"name":"OSPF","slug":"OSPF","permalink":"https://delacroix.github.io/tags/OSPF/"}]},{"title":"常用Centos命令","slug":"常用Centos命令","date":"2016-11-04T02:56:52.000Z","updated":"2016-11-07T01:44:54.327Z","comments":true,"path":"2016/11/04/常用Centos命令/","link":"","permalink":"https://delacroix.github.io/2016/11/04/常用Centos命令/","excerpt":"安装方式：• yum安装[需联网]yum install vim [yum方式安装vim软件及相关依赖包，如有更新包，则提示是否更新]yum search vim [yum方式搜索vim软件是否存在]yum list |grep vim [yum方式列出vim安装状态、版本及包全名]","text":"安装方式：• yum安装[需联网]yum install vim [yum方式安装vim软件及相关依赖包，如有更新包，则提示是否更新]yum search vim [yum方式搜索vim软件是否存在]yum list |grep vim [yum方式列出vim安装状态、版本及包全名] • rpm安装rpm -ivh vim-minimal.x86_64.rpm [rpm方式安装vim软件，如差依赖包，会有提示]rpm -qa |grep vim [查询系统是否安装vim软件，有则显示包全名及版本号]rpm -ql yum [列出yum软件所安装的文件]• 源码安装[常规安装方式]./configure 配置源码包相关安装 –help 打印帮助文档make 编译代码make install 安装软件及输出安装信息 注：Centos6及以下系统系统使用的pyton2.6，Centos7使用的python2.7，yum命令会使用python命令，系统版本对应python版本，请勿随意升级 查看端口：• netstat[Centos6及以下]netstat -nltp [打印TCP协议，状态为LISTEN的端口信息，并打印PID号]netstat -nultp [打印TCP、UDP协议，状态为LISTEN的端口信息，并打印PID号]netstat -anp [打印TCP、UDP、SOCKET协议所有状态，并打印PID号]netstat -rn [打印路由表信息]netstat -anp |awk ‘$1==”tcp”{print $6}’ |sort -n |uniq -c |sort -n [打印TCP连接状态] 注：UDP无LISTEN状态、LISTEN状态仅适用于TCP • ss[Centos7]ss命令与netstat命令基本类似，参考以上命令 查看网络地址：• ifconfig[Centos6及以下]ifconfig -a [打印所有网络设备信息]ifconfig eth0 [打印eth0网卡信息]ifconfig eth0 up/down [启用或停用eth0网卡]ifconfig eth0 mtu 1316 [更改eth0网卡MTU值为1316] 注：建立VPN服务器，如果客户机不能上网，需确认MTU值是否正确 • ip[Centos7]ip命令与ifconfig命令基本类似，参考以上命令• routeroute -n [打印路由表信息] 注：添加/删除路由表需使用route命令 查看进程：• psps aux |grep bash[查看bash进程信息，打印启动进程用户，PID，启动命令等]ps -ef |grep bash [查看bash进程信息，打印UID，PID，PPID，启动命令等]• pstreepstree -a [打印进程之间子父关系]• toptop M [以兆字节显示系统硬件相关信息] 查看文件占用：• lsoflsof -Pi :10086 [查看10086端口连接情况，打印启动进程用户，PID，协议，命令等]lsof -p 19523 [查看19523进程号信息，打印启动进程用户，PID，命令，类型，调用文件或目录情况等]lsof redis.sh [查看redis.sh文件被调用情况，打印命令，PID，启动进程用户等]lsof +d /mnt [查看/mnt目录被调用情况，打印命令，PID，启动进程用户，启动命令等]lsof +D /mnt [查看/mnt目录被调用情况，打印命令，PID，启动进程用户，启动命令等] 注：小写d不包含子目录，大写D包含子目录下文件 • fuserfuser -nv tcp 53 [打印TCP协议端口为53的相关信息]fuser -v 53/tcp [同上]• lddldd -v /usr/bin/vim [打印vim命令共享库调用情况] 注：太多命令会调用GLIBC库，请勿随意升级或覆盖安装 查找文件：• findfind . -type f -name “*tw*” [查找当前目录及子目录下文件名含有tw的文件]find / -maxdepth 3 -type f -name “*.log” [查找/目录下文件名为log结尾的文件，深度只查找到第3层]find . -type d -name “*log*” [查找当前目录及子目录文件夹名含有log的文件夹] 查找文件内容：• grepgrep “dir” * [查找当前目录所有含有dir字符的文件，不扫描子目录]• find+grepfind . |xargs grep “path” [查找当前目录及子目录所有文件含有path字符的文件] 注：请写明具体路径，不要直接填写为/，这会导致IO和CPU占用过高","categories":[{"name":"Linux","slug":"Linux","permalink":"https://delacroix.github.io/categories/Linux/"}],"tags":[{"name":"基础","slug":"基础","permalink":"https://delacroix.github.io/tags/基础/"}]},{"title":"LOGSTASH+ELASTICSEARCH 处理 MYSQL 慢查询日志","slug":"LOGSTASH-ELASTICSEARCH-处理-MYSQL-慢查询日志","date":"2016-11-04T02:08:23.000Z","updated":"2016-11-07T01:44:17.671Z","comments":true,"path":"2016/11/04/LOGSTASH-ELASTICSEARCH-处理-MYSQL-慢查询日志/","link":"","permalink":"https://delacroix.github.io/2016/11/04/LOGSTASH-ELASTICSEARCH-处理-MYSQL-慢查询日志/","excerpt":"LOGSTASH+ELASTICSEARCH 处理 MYSQL 慢查询日志先确认慢查询日志是否开启, 然后找到日志文件的位置 123456789&gt; show variables like '%slow%';+---------------------+-------------------------------------+| Variable_name | Value |+---------------------+-------------------------------------+| log_slow_queries | ON || slow_launch_time | 2 || slow_query_log | ON || slow_query_log_file | /data/mysqllog/20000/slow-query.log |+---------------------+-------------------------------------+","text":"LOGSTASH+ELASTICSEARCH 处理 MYSQL 慢查询日志先确认慢查询日志是否开启, 然后找到日志文件的位置 123456789&gt; show variables like '%slow%';+---------------------+-------------------------------------+| Variable_name | Value |+---------------------+-------------------------------------+| log_slow_queries | ON || slow_launch_time | 2 || slow_query_log | ON || slow_query_log_file | /data/mysqllog/20000/slow-query.log |+---------------------+-------------------------------------+ 2. 慢查询日志格式基本如下, 当然, 格式如果有差异, 需要根据具体格式进行小的修改 123456# Time: 160524 5:12:29# User@Host: user_a[xxxx] @ [10.166.140.109]# Query_time: 1.711086 Lock_time: 0.000040 Rows_sent: 385489 Rows_examined: 385489use dbname;SET timestamp=1464037949;SELECT 1 from dbname; 3. 使用 logstash 采集采集, 无非是用multiline进行多行解析但是, 需要处理的第一, 去除没用的信息第二, 慢查询 sql, 是会反复出现的, 所以, 执行次数成了一个很重要的指标. 我们要做的, 就是降噪(将参数去掉, 涉及带引号的内容 + 数字), 将参数类信息过滤掉, 留下核心的 sql, 然后计算出一个 hash, 这样就可以在查询, 根据这个字段进行聚合. 这里用到了 mutate 以及 checksum 1234567891011121314# calculate unique hash mutate &#123; add_field =&gt; &#123;\"sql_for_hash\" =&gt; \"%&#123;sql&#125;\"&#125; &#125; mutate &#123; gsub =&gt; [ \"sql_for_hash\", \"'.+?'\", \"\", \"sql_for_hash\", \"-?\\d*\\.&#123;0,1&#125;\\d+\", \"\" ] &#125; checksum &#123; algorithm =&gt; \"md5\" keys =&gt; [\"sql_for_hash\"] &#125; 最后算出来的 md5, 放入了logstash_checksum完整的 logstash 配置文件 (具体使用可能需要根据自身日志格式做些小调整) 注意, 里面的 patternALLWORD [\\s\\S]* 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192input &#123; file &#123; path =&gt; [\"/data/mysqllog/20000/slow-query.log\"] sincedb_path =&gt; \"/data/LogNew/logstash/sincedb/mysql.sincedb\" type =&gt; \"mysql-slow-log\" add_field =&gt; [\"env\", \"PRODUCT\"] codec =&gt; multiline &#123; pattern =&gt; \"^# User@Host:\" negate =&gt; true what =&gt; previous &#125; &#125;&#125;filter &#123; grok &#123; # User@Host: logstash[logstash] @ localhost [127.0.0.1] # User@Host: logstash[logstash] @ [127.0.0.1] match =&gt; [ \"message\", \"^# User@Host: %&#123;ALLWORD:user&#125;\\[%&#123;ALLWAORD&#125;\\] @ %&#123;ALLWORD:dbhost&#125;? \\[%&#123;IP:ip&#125;\\]\" ] &#125; grok &#123; # Query_time: 102.413328 Lock_time: 0.000167 Rows_sent: 0 Rows_examined: 1970 match =&gt; [ \"message\", \"^# Query_time: %&#123;NUMBER:duration:float&#125;%&#123;SPACE&#125;Lock_time: %&#123;NUMBER:lock_wait:float&#125;%&#123;SPACE&#125;Rows_sent: %&#123;NUMBER:results:int&#125;%&#123;SPACE&#125;Rows_examined:%&#123;SPACE&#125;%&#123;NUMBER:scanned:int&#125;%&#123;ALLWORD:sql&#125;\"] &#125; // remove useless data mutate &#123; gsub =&gt; [ \"sql\", \"\\nSET timestamp=\\d+?;\\n\", \"\", \"sql\", \"\\nuse [a-zA-Z0-9\\-\\_]+?;\", \"\", \"sql\", \"\\n# Time: \\d+\\s+\\d+:\\d+:\\d+\", \"\", \"sql\", \"\\n/usr/local/mysql/bin/mysqld.+$\", \"\", \"sql\", \"\\nTcp port:.+$\", \"\", \"sql\", \"\\nTime .+$\", \"\" ] &#125; # Capture the time the query happened grok &#123; match =&gt; [ \"message\", \"^SET timestamp=%&#123;NUMBER:timestamp&#125;;\" ] &#125; date &#123; match =&gt; [ \"timestamp\", \"UNIX\" ] &#125; # calculate unique hash mutate &#123; add_field =&gt; &#123;\"sql_for_hash\" =&gt; \"%&#123;sql&#125;\"&#125; &#125; mutate &#123; gsub =&gt; [ \"sql_for_hash\", \"'.+?'\", \"\", \"sql_for_hash\", \"-?\\d*\\.&#123;0,1&#125;\\d+\", \"\" ] &#125; checksum &#123; algorithm =&gt; \"md5\" keys =&gt; [\"sql_for_hash\"] &#125; # Drop the captured timestamp field since it has been moved to the time of the event mutate &#123; # TODO: remove the message field remove_field =&gt; [\"timestamp\", \"message\", \"sql_for_hash\"] &#125;&#125;output &#123; #stdout&#123; # codec =&gt; rubydebug #&#125; #if (\"_grokparsefailure\" not in [tags]) &#123; # stdout&#123; # codec =&gt; rubydebug # &#125; #&#125; if (\"_grokparsefailure\" not in [tags]) &#123; elasticsearch &#123; hosts =&gt; [\"192.168.1.1:9200\"] index =&gt; \"logstash-slowlog\" &#125; &#125;&#125;采集进去的内容 &#123; \"@timestamp\" =&gt; \"2016-05-23T21:12:59.000Z\", \"@version\" =&gt; \"1\", \"tags\" =&gt; [ [0] \"multiline\" ], \"path\" =&gt; \"/Users/ken/tx/elk/logstash/data/slow_sql.log\", \"host\" =&gt; \"Luna-mac-2.local\", \"type\" =&gt; \"mysql-slow\", \"env\" =&gt; \"PRODUCT\", \"user\" =&gt; \"dba_bak_all_sel\", \"ip\" =&gt; \"10.166.140.109\", \"duration\" =&gt; 28.812601, \"lock_wait\" =&gt; 0.000132, \"results\" =&gt; 749414, \"scanned\" =&gt; 749414, \"sql\" =&gt; \"SELECT /*!40001 SQL_NO_CACHE */ * FROM `xxxxx`;\", \"logstash_checksum\" =&gt; \"3e3ccb89ee792de882a57e2bef6c5371\"&#125; 4. 写查询查询, 我们需要按logstash_checksum进行聚合, 然后按照次数由多到少降序展示, 同时, 每个logstash_checksum需要有一条具体的 sql 进行展示通过 es 的 Top hits Aggregation 可以完美地解决这个查询需求查询的 query 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748 body = &#123; \"from\": 0, \"size\": 0, \"query\": &#123; \"filtered\": &#123; \"query\": &#123; \"match\": &#123; \"user\": \"test\" &#125; &#125;, \"filter\": &#123; \"range\": &#123; \"@timestamp\": &#123; \"gte\": \"now-1d\", \"lte\": \"now\" &#125; &#125; &#125; &#125; &#125;, \"aggs\": &#123; \"top_errors\": &#123; \"terms\": &#123; \"field\": \"logstash_checksum\", \"size\": 20 &#125;, \"aggs\": &#123; \"top_error_hits\": &#123; \"top_hits\": &#123; \"sort\": [ &#123; \"@timestamp\":&#123; \"order\": \"desc\" &#125; &#125; ], \"_source\": &#123; \"include\": [ \"user\" , \"sql\", \"logstash_checksum\", \"@timestamp\", \"duration\", \"lock_wait\", \"results\", \"scanned\" ] &#125;, \"size\" : 1 &#125; &#125; &#125; &#125; &#125;&#125; 跟这个写法相关的几个参考链接: Terms Aggregation / Elasticsearch filter document group by field 5. 渲染页面python 的后台, 使用sqlparse包, 将 sql 进行格式化 (换行 / 缩进 / 大小写), 再往前端传. sqlparse 1234567&gt;&gt;&gt; sql = 'select * from foo where id in (select id from bar);'&gt;&gt;&gt; print sqlparse.format(sql, reindent=True, keyword_case='upper')SELECT *FROM fooWHERE id IN (SELECT id FROM bar);","categories":[{"name":"Linux","slug":"Linux","permalink":"https://delacroix.github.io/categories/Linux/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"https://delacroix.github.io/tags/ELK/"},{"name":"MySQL","slug":"MySQL","permalink":"https://delacroix.github.io/tags/MySQL/"}]},{"title":"kubeadm 搭建 kubernetes 集群","slug":"kubeadm-搭建-kubernetes-集群","date":"2016-11-03T05:16:44.000Z","updated":"2016-11-07T01:43:55.221Z","comments":true,"path":"2016/11/03/kubeadm-搭建-kubernetes-集群/","link":"","permalink":"https://delacroix.github.io/2016/11/03/kubeadm-搭建-kubernetes-集群/","excerpt":"[转]kubeadm 搭建 kubernetes 集群原作者链接：点我 距离上一篇 kubernetes 1.4 集群搭建 发布间隔不算太久，自己也不断地在生产和测试环境鼓捣，有不少 “逗比” 的经历，准备写一下具体的 kubeadm 搭建集群的一些坑和踩坑的经验，如果没有使用过 kubeadm 的同学，最好先看下上面的文章，然后鼓捣一遍，也许并不会成功，但大部分坑再来看此文会有收获 1 环境准备首先环境还是三台虚拟机，虚拟机地址如下 IP 地址 节点 192.168.1.167 master 192.168.1.189 node1 192.168.1.189 node2","text":"[转]kubeadm 搭建 kubernetes 集群原作者链接：点我 距离上一篇 kubernetes 1.4 集群搭建 发布间隔不算太久，自己也不断地在生产和测试环境鼓捣，有不少 “逗比” 的经历，准备写一下具体的 kubeadm 搭建集群的一些坑和踩坑的经验，如果没有使用过 kubeadm 的同学，最好先看下上面的文章，然后鼓捣一遍，也许并不会成功，但大部分坑再来看此文会有收获 1 环境准备首先环境还是三台虚拟机，虚拟机地址如下 IP 地址 节点 192.168.1.167 master 192.168.1.189 node1 192.168.1.189 node2 然后每台机器安装好 docker，至于 rpm 安装包版本下面介绍 2 说点正经事2.1 安装包从哪来官方的文档页面更新并不及时，同时他的 yum 源更新也很慢，再者…那他妈可是 Google 的服务器，能特么连上吗？以前总是在国外服务器使用 yumdownloader 下载，然后 scp 到本地，虽然能解决问题，但是蛋碎一地…最后找到了源头，如下 Kubernetes 编译的各种发行版安装包来源于 Github 上的另一个叫 release 的项目，地址 点这里，把这个项目 clone 下来，由于本人是 Centos 用户，所以进入 rpm 目录，在安装好 docker 的机器上执行那个 docker-build.sh 脚本即可编译 rpm 包，最后会生成到当前目录的 output 目录下,截图如下 2.2 镜像从哪来对的，没错，gcr.io 就是 Google 的域名，服务器更不用提，所以在进行 kubeadm init 操作时如果不先把这些镜像 load 进去绝对会卡死不动，以下列出了所需镜像，但是版本号根据 rpm 版本不同可能略有不同，具体怎么看下面介绍 镜像名称 版本号 gcr.io/google_containers/kube-discovery-amd64 1.0 gcr.io/google_containers/kubedns-amd64 1.7 gcr.io/google_containers/kube-proxy-amd64 v1.4.1 gcr.io/google_containers/kube-scheduler-amd64 v1.4.1 gcr.io/google_containers/kube-controller-manager-amd64 v1.4.1 gcr.io/google_containers/kube-apiserver-amd64 v1.4.1 gcr.io/google_containers/etcd-amd64 2.2.5 gcr.io/google_containers/kube-dnsmasq-amd64 1.3 gcr.io/google_containers/exechealthz-amd64 1.1 gcr.io/google_containers/pause-amd64 3.0 这些镜像有两种办法可以获取，第一种是利用一台国外的服务器，在上面 pull 下来，然后再 save 成 tar 文件，最后 scp 到本地 load 进去；相对于第一种方式比较坑的是取决于服务器速度，每次搞起来也很蛋疼，第二种方式就是利用 docker hub 做中转，简单的说就是利用 docker hub 的自动构建功能，在 Github 中创建一个 Dockerfile，里面只需要 FROM xxxx 这些 gcr.io 的镜像即可，最后 pull 到本地，然后再 tag 一下 首先创建一个 github 项目，可以直接 fork 我的即可 其中每个 Dockerfile 只需要 FROM 一下即可 最后在 Docker Hub 上创建自动构建项目 最后要手动触发一下，然后 Docker Hub 才会开始给你编译 等待完成即可直接 pull 了 2.3 镜像版本怎么整上面已经解决了镜像获取问题，但是一大心病就是 “我特么怎么知道是哪个版本的”，为了发扬 “刨根问底” 的精神，先进行一遍 kubeadm init，这时候绝对卡死，此时进入 /etc/kubernetes/manifests 可以看到许多 json 文件，这些文件中定义了需要哪些基础镜像 从上图中基本可以看到 kubeadm init 的时候会拉取哪些基础镜像了，但是还有一些镜像，仍然无法找到，比如kubedns、pause 等，至于其他的镜像版本，可以从源码中找到，源码位置是 kubernetes/cmd/kubeadm/app/images/images.go 这个文件中，如下所示: 剩余的一些镜像，比如 kube-proxy-amd64、kube-discovery-amd64 两个镜像，其中 kube-discovery-amd64 现在一直是 1.0 版本，源码如下所示 kube-proxy-amd64 则是一直跟随基础组件的主版本，也就是说如果从 manifests 中看到 controller 等版本是 v.1.4.4，那么 kube-proxy-amd64 也是这个版本，源码如下 最后根据这些版本去 github 上准备相应的 Dockerfile，在利用 Docker Hub 的自动构建 build 一下，再 pull 下来 tag 成对应的镜像名称即可 三 搭建集群3.1 主机名处理经过亲测，节点主机名最好为 xxx.xxx 这种域名格式，否则在某些情况下，POD 中跑的程序使用域名解析时可能出现问题，所以先要处理一下主机名 123456789#写入 hostname(node 节点后缀改成 .node)echo \"192-168-1-167.master\" &gt; /etc/hostname #加入 hostsecho \"127.0.0.1 192-168-1-167.master\" &gt;&gt; /etc/hosts#不重启情况下使内核生效sysctl kernel.hostname=192-168-1-167.master#验证是否修改成功➜ ~ hostname192-168-1-167.master 3.2 load 镜像由于本人已经在 Docker Hub 上处理好了相关镜像，所以直接 pull 下来 tag 一下即可， 123456images=(kube-proxy-amd64:v1.4.4 kube-discovery-amd64:1.0 kubedns-amd64:1.7 kube-scheduler-amd64:v1.4.4 kube-controller-manager-amd64:v1.4.4 kube-apiserver-amd64:v1.4.4 etcd-amd64:2.2.5 kube-dnsmasq-amd64:1.3 exechealthz-amd64:1.1 pause-amd64:3.0 kubernetes-dashboard-amd64:v1.4.1)for imageName in $&#123;images[@]&#125; ; do docker pull mritd/$imageName docker tag mritd/$imageName gcr.io/google_containers/$imageName docker rmi mritd/$imageNamedone 3.3 安装 rpmrpm 获取办法上文已经提到，可以自己编译，这里我已经编译好并维护了一个 yum 源，直接yum install 即可(懒) 12345678910111213#添加 yum 源tee /etc/yum.repos.d/mritd.repo &lt;&lt; EOF[mritdrepo]name=Mritd Repositorybaseurl=https://rpm.mritd.me/centos/7/x86_64enabled=1gpgcheck=1gpgkey=https://cdn.mritd.me/keys/rpm.public.keyEOF#刷新cacheyum makecache#安装yum install -y kubelet kubectl kubernetes-cni kubeadm 3.4 初始化 master等会有个坑，kubeadm 等相关 rpm 安装后会生成 /etc/kubernetes 目录，而 kubeadm init 时候又会检测这些目录是否存在，如果存在则停止初始化，所以要先清理一下，以下清理脚本来源于 官方文档 Tear down 部分，该脚本同样适用于初始化失败进行重置 1234567systemctl stop kubelet;#注意: 下面这条命令会干掉所有正在运行的 docker 容器，#如果要进行重置操作，最好先确定当前运行的所有容器都能干掉(干掉不影响业务)，#否则的话最好手动删除 kubeadm 创建的相关容器(gcr.io 相关的)docker rm -f -v $(docker ps -q);find /var/lib/kubelet | xargs -n 1 findmnt -n -t tmpfs -o TARGET -T | uniq | xargs -r umount -v;rm -r -f /etc/kubernetes /var/lib/kubelet /var/lib/etcd; 还有个坑，初始化以前记得一定要启动 kubelet，虽然你 systemctl status kubelet 看着他是启动失败，但是也得启动，否则绝壁卡死 12systemctl enable kubeletsystemctl start kubelet 等会等会，还有坑，新版本直接 init 会提示 ebtables not found in system path 错误，所以还得先安装一下这个包在初始化 12#安装 ebtablesyum install -y ebtables 最后见证奇迹的时刻 12#初始化并指定 apiserver 监听地址kubeadm init --api-advertise-addresses 192.168.1.167 完美截图如下 这里再爆料一个坑，底下的 kubeadm join –token=b17964.5d8a3c14e99cf6aa 192.168.1.167 这条命令一定保存好，因为后期没法重现的，你们老大再让你添加机器的时候如果没这个你会哭的 3.5 加入 node上面所有坑大约说的差不多了，直接上命令了 处理主机名 123echo \"192-168-1-189.node\" &gt; /etc/hostname echo \"127.0.0.1 192-168-1-189.node\" &gt;&gt; /etc/hostssysctl kernel.hostname=192-168-1-189.node 拉取镜像 123456images=(kube-proxy-amd64:v1.4.4 kube-discovery-amd64:1.0 kubedns-amd64:1.7 kube-scheduler-amd64:v1.4.4 kube-controller-manager-amd64:v1.4.4 kube-apiserver-amd64:v1.4.4 etcd-amd64:2.2.5 kube-dnsmasq-amd64:1.3 exechealthz-amd64:1.1 pause-amd64:3.0 kubernetes-dashboard-amd64:v1.4.1)for imageName in $&#123;images[@]&#125; ; do docker pull mritd/$imageName docker tag mritd/$imageName gcr.io/google_containers/$imageName docker rmi mritd/$imageNamedone 装 rpm 12345678910tee /etc/yum.repos.d/mritd.repo &lt;&lt; EOF[mritdrepo]name=Mritd Repositorybaseurl=https://rpm.mritd.me/centos/7/x86_64enabled=1gpgcheck=1gpgkey=https://cdn.mritd.me/keys/rpm.public.keyEOFyum makecacheyum install -y kubelet kubectl kubernetes-cni kubeadm ebtables 清理目录(没初始化过只需要删目录) 1rm -r -f /etc/kubernetes /var/lib/kubelet /var/lib/etcd; 启动 kubelet 12systemctl enable kubeletsystemctl start kubelet 初始化加入集群 1kubeadm join --token=b17964.5d8a3c14e99cf6aa 192.168.1.167 同样完美截图 3.6 部署 weave 网络再没部署 weave 时，dns 是启动不了的，如下 官方给出的命令是这样的 1kubectl create -f https://git.io/weave-kube 本着 “刨根问底挖祖坟” 的精神，先把这个 yaml 搞下来 1wget https://git.io/weave-kube -O weave-kube.yaml 然后同样的套路，打开看一下镜像，利用 Docker Hub 做中转，搞下来再 load 进去，然后 create -f 就行了 1234docker pull mritd/weave-kube:1.7.2docker tag mritd/weave-kube:1.7.2 weaveworks/weave-kube:1.7.2docker rmi mritd/weave-kube:1.7.2kubectl create -f weave-kube.yaml 完美截图 3.7 部署 dashboarddashboard 的命令也跟 weave 的一样，不过有个大坑，默认的 yaml 文件中对于 image 拉取策略的定义是 无论何时都会去拉取镜像，导致即使你 load 进去也无卵用，所以还得先把 yaml 搞下来然后改一下镜像拉取策略，最后再 create -f 即可 1wget https://rawgit.com/kubernetes/dashboard/master/src/deploy/kubernetes-dashboard.yaml -O kubernetes-dashboard.yaml 编辑 yaml 改一下 imagePullPolicy，把 Always 改成 IfNotPresent(本地没有再去拉取) 或者 Never(从不去拉取) 即可 最后再利用 Dokcer Hub 中转，然后创建(实际上 dashboard 已经有了 v1.4.1，我这里已经改了) 1kubectl create -f kubernetes-dashboard.yaml 截图如下 通过 describe 命令我们可以查看其暴露出的 NodePoint,然后便可访问 四 其他的一些坑还有一些其他的坑等着大家去摸索，其中有一个是 DNS 解析错误，表现形式为 POD 内的程序通过域名访问解析不了，cat 一下容器的 /etc/resolv.conf发现指向的 dns 服务器与 kubectl get svc –namespace=kube-system 中的 kube-dsn 地址不符；解决办法就是 编辑节点的 /etc/systemd/system/kubelet.service.d/10-kubeadm.conf 文件，更改 KUBELET_DNS_ARGS 地址为 get svc 中的 kube-dns 地址，然后重启 kubelet 服务，重新杀掉 POD 让 kubernetes 重建即可 其他坑欢迎大家补充","categories":[{"name":"Linux","slug":"Linux","permalink":"https://delacroix.github.io/categories/Linux/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://delacroix.github.io/tags/Kubernetes/"},{"name":"Docker","slug":"Docker","permalink":"https://delacroix.github.io/tags/Docker/"}]},{"title":"LVS集群中的IP负载均衡技术","slug":"LVS集群中的IP负载均衡技术-1","date":"2016-11-03T02:55:24.000Z","updated":"2016-11-07T01:44:27.022Z","comments":true,"path":"2016/11/03/LVS集群中的IP负载均衡技术-1/","link":"","permalink":"https://delacroix.github.io/2016/11/03/LVS集群中的IP负载均衡技术-1/","excerpt":"[转]LVS集群中的IP负载均衡技术 本文在分析服务器集群实现虚拟网络服务的相关技术上，详细描述了LVS集群中实现的三种IP负载均衡技术（VS/NAT、VS/TUN和VS/DR）的工作原理，以及它们的优缺点。 1 前言在前面文章中，讲述了可伸缩网络服务的几种结构，它们都需要一个前端的负载调度器（或者多个进行主从备份）。我们先分析实现虚拟网络服务的主要技术，指出 IP负载均衡技术是在负载调度器的实现技术中效率最高的。在已有的IP负载均衡技术中，主要有通过网络地址转换（Network Address Translation）将一组服务器构成一个高性能的、高可用的虚拟服务器，我们称之为VS/NAT技术（Virtual Server via Network Address Translation）。在分析VS/NAT的缺点和网络服务的非对称性的基础上，我们提出了通过IP隧道实现虚拟服务器的方法VS/TUN （Virtual Server via IP Tunneling），和通过直接路由实现虚拟服务器的方法VS/DR（Virtual Server via Direct Routing），它们可以极大地提高系统的伸缩性。VS/NAT、VS/TUN和VS/DR技术是LVS集群中实现的三种IP负载均衡技术，我们将在文 章中详细描述它们的工作原理和各自的优缺点。 在以下描述中，我们称客户的socket和服务器的socket之间的数据通讯为连接，无论它们是使用TCP还是UDP协议。下面简述当前用服务器集群实现高可伸缩、高可用网络服务的几种负载调度方法，并列举几个在这方面有代表性的研究项目。","text":"[转]LVS集群中的IP负载均衡技术 本文在分析服务器集群实现虚拟网络服务的相关技术上，详细描述了LVS集群中实现的三种IP负载均衡技术（VS/NAT、VS/TUN和VS/DR）的工作原理，以及它们的优缺点。 1 前言在前面文章中，讲述了可伸缩网络服务的几种结构，它们都需要一个前端的负载调度器（或者多个进行主从备份）。我们先分析实现虚拟网络服务的主要技术，指出 IP负载均衡技术是在负载调度器的实现技术中效率最高的。在已有的IP负载均衡技术中，主要有通过网络地址转换（Network Address Translation）将一组服务器构成一个高性能的、高可用的虚拟服务器，我们称之为VS/NAT技术（Virtual Server via Network Address Translation）。在分析VS/NAT的缺点和网络服务的非对称性的基础上，我们提出了通过IP隧道实现虚拟服务器的方法VS/TUN （Virtual Server via IP Tunneling），和通过直接路由实现虚拟服务器的方法VS/DR（Virtual Server via Direct Routing），它们可以极大地提高系统的伸缩性。VS/NAT、VS/TUN和VS/DR技术是LVS集群中实现的三种IP负载均衡技术，我们将在文 章中详细描述它们的工作原理和各自的优缺点。 在以下描述中，我们称客户的socket和服务器的socket之间的数据通讯为连接，无论它们是使用TCP还是UDP协议。下面简述当前用服务器集群实现高可伸缩、高可用网络服务的几种负载调度方法，并列举几个在这方面有代表性的研究项目。 2 实现虚拟服务的相关方法在网络服务中，一端是客户程序，另一端是服务程序，在中间可能有代理程序。由此看来，可以在不同的层次上实现多台服务器的负载均衡。用集群解决网络服务性能问题的现有方法主要分为以下四类。 2.1 基于RR-DNS的解决方法NCSA的可伸缩的WEB服务器系统就是最早基于RR-DNS（Round-Robin Domain Name System）的原型系统[1,2]。它的结构和工作流程如下图所示： 有一组WEB服务器，他们通过分布式文件系统AFS(Andrew File System)来共享所有的HTML文档。这组服务器拥有相同的域名（如www.ncsa.uiuc.edu），当用户按照这个域名访问时, RR-DNS服务器会把域名轮流解析到这组服务器的不同IP地址，从而将访问负载分到各台服务器上。 这种方法带来几个问题。第一，域名服务器是一个分布式系统，是按照一定的层次结构组织的。当用户就域名解析请求提交给本地的域名服务器，它会因不能直接解析而向上一级域名服务器提交，上一级域 名服务器再依次向上提交，直到RR-DNS域名服器把这个域名解析到其中一台服务器的IP地址。可见，从用户到RR-DNS间存在多台域名服器，而它们都 会缓冲已解析的名字到IP地址的映射,这会导致该域名服器组下所有用户都会访问同一WEB服务器，出现不同WEB服务器间严重的负载不平衡。为了保证在域 名服务器中域名到IP地址的映射不被长久缓冲，RR-DNS在域名到IP地址的映射上设置一个TTL(Time To Live)值，过了这一段时间，域名服务器将这个映射从缓冲中淘汰。当用户请求，它会再向上一级域名服器提交请求并进行重新影射。这就涉及到如何设置这个 TTL值，若这个值太大，在这个TTL期间，很多请求会被映射到同一台WEB服务器上，同样会导致严重的负载不平衡。若这个值太小，例如是0，会导致本地域名服务器频繁地向RR-DNS提交请求，增加了域名解析的网络流量，同样会使RR-DNS服务器成为系统中一个新的瓶颈。 第二，用户机器会缓冲从名字到IP地址的映射，而不受TTL值的影响，用户的访问请求会被送到同一台WEB服务器上。由于用户访问请求的突发性和访问方式不同，例如有的 人访问一下就离开了，而有的人访问可长达几个小时，所以各台服务器间的负载仍存在倾斜（Skew）而不能控制。假设用户在每个会话中平均请求数为20，负 载最大的服务器获得的请求数额高于各服务器平均请求数的平均比率超过百分之三十。也就是说，当TTL值为0时，因为用户访问的突发性也会存在着较严重的负 载不平衡。 第三，系统的可靠性和可维护性差。若一台服务器失效，会导致将域名解析到该服务器的用户看到服务中断，即使用户按 “Reload”按钮，也无济于事。系统管理员也不能随时地将一台服务器切出服务进行系统维护，如进行操作系统和应用软件升级，这需要修改RR-DNS服 务器中的IP地址列表，把该服务器的IP地址从中划掉，然后等上几天或者更长的时间，等所有域名服器将该域名到这台服务器的映射淘汰，和所有映射到这台服 务器的客户机不再使用该站点为止。 2.2 基于客户端的解决方法基于客户端的解决方法需要每个客户程序都有一定的服务器集群的知识，进而把以负载均衡的方式将请求发到不同的服务器。例如，Netscape Navigator浏览器访问Netscape的主页时，它会随机地从一百多台服务器中挑选第N台，最后将请求送往wwwN.netscape.com。 然而，这不是很好的解决方法，Netscape只是利用它的Navigator避免了RR-DNS解析的麻烦，当使用IE等其他浏览器不可避免的要进行 RR-DNS解析。 Smart Client[3]是Berkeley做的另一种基于客户端的解决方法。服务提供一个Java Applet在客户方浏览器中运行，Applet向各个服务器发请求来收集服务器的负载等信息，再根据这些信息将客户的请求发到相应的服务器。高可用性也 在Applet中实现，当服务器没有响应时，Applet向另一个服务器转发请求。这种方法的透明性不好，Applet向各服务器查询来收集信息会增加额 外的网络流量，不具有普遍的适用性。 2.3 基于应用层负载均衡调度的解决方法多台服务器通过高速的互联网络连接成一个集群系统，在前端有一个基于应用层的负载调度器。当用户访问请求到达调度器时，请求会提交给作负载均衡调度的应用程 序，分析请求，根据各个服务器的负载情况，选出一台服务器，重写请求并向选出的服务器访问，取得结果后，再返回给用户。 应用层负载均衡调度 的典型代表有Zeus负载调度器[4]、pWeb[5]、Reverse-Proxy[6]和SWEB[7]等。Zeus负载调度器是Zeus公司的商业 产品，它是在Zeus Web服务器程序改写而成的，采用单进程事件驱动的服务器结构。pWeb就是一个基于Apache 1.1服务器程序改写而成的并行WEB调度程序，当一个HTTP请求到达时，pWeb会选出一个服务器，重写请求并向这个服务器发出改写后的请求，等结果 返回后，再将结果转发给客户。Reverse-Proxy利用Apache 1.3.1中的Proxy模块和Rewrite模块实现一个可伸缩WEB服务器，它与pWeb的不同之处在于它要先从Proxy的cache中查找后，若 没有这个副本，再选一台服务器，向服务器发送请求，再将服务器返回的结果转发给客户。SWEB是利用HTTP中的redirect错误代码，将客户请求到 达一台WEB服务器后，这个WEB服务器根据自己的负载情况，自己处理请求，或者通过redirect错误代码将客户引到另一台WEB服务器，以实现一个 可伸缩的WEB服务器。 基于应用层负载均衡调度的多服务器解决方法也存在一些问题。第一，系统处理开销特别大，致使系统的伸缩性有限。当请 求到达负载均衡调度器至处理结束时，调度器需要进行四次从核心到用户空间或从用户空间到核心空间的上下文切换和内存复制；需要进行二次TCP连接，一次是 从用户到调度器，另一次是从调度器到真实服务器；需要对请求进行分析和重写。这些处理都需要不小的ＣＰＵ、内存和网络等资源开销，且处理时间长。所构成系 统的性能不能接近线性增加的，一般服务器组增至3或4台时，调度器本身可能会成为新的瓶颈。所以，这种基于应用层负载均衡调度的方法的伸缩性极其有限。第 二，基于应用层的负载均衡调度器对于不同的应用，需要写不同的调度器。以上几个系统都是基于HTTP协议，若对于FTP、Mail、POP3等应用，都需 要重写调度器。 2.4 基于IP层负载均衡调度的解决方法用户通过虚拟IP地址（Virtual IP Address）访问服务时，访问请求的报文会到达负载调度器，由它进行负载均衡调度，从一组真实服务器选出一个，将报文的目标地址Virtual IP Address改写成选定服务器的地址，报文的目标端口改写成选定服务器的相应端口，最后将报文发送给选定的服务器。真实服务器的回应报文经过负载调度器 时，将报文的源地址和源端口改为Virtual IP Address和相应的端口，再把报文发给用户。Berkeley的MagicRouter[8]、Cisco的LocalDirector、 Alteon的ACEDirector和F5的Big/IP等都是使用网络地址转换方法。MagicRouter是在Linux 1.3版本上应用快速报文插入技术，使得进行负载均衡调度的用户进程访问网络设备接近核心空间的速度，降低了上下文切换的处理开销，但并不彻底，它只是研 究的原型系统，没有成为有用的系统存活下来。Cisco的LocalDirector、Alteon的ACEDirector和F5的Big/IP是非常 昂贵的商品化系统，它们支持部分TCP/UDP协议，有些在ICMP处理上存在问题。 IBM的TCP Router[9]使用修改过的网络地址转换方法在SP/2系统实现可伸缩的WEB服务器。TCP Router修改请求报文的目标地址并把它转发给选出的服务器，服务器能把响应报文的源地址置为TCP Router地址而非自己的地址。这种方法的好处是响应报文可以直接返回给客户，坏处是每台服务器的操作系统内核都需要修改。IBM的 NetDispatcher[10]是TCP Router的后继者，它将报文转发给服务器，而服务器在non-ARP的设备配置路由器的地址。这种方法与LVS集群中的VS/DR类似，它具有很高的 可伸缩性，但一套在IBM SP/2和NetDispatcher需要上百万美金。总的来说，IBM的技术还挺不错的。 在贝尔实验室的 ONE-IP[11]中，每台服务器都独立的IP地址，但都用IP Alias配置上同一VIP地址，采用路由和广播两种方法分发请求，服务器收到请求后按VIP地址处理请求，并以VIP为源地址返回结果。这种方法也是为 了避免回应报文的重写，但是每台服务器用IP Alias配置上同一VIP地址，会导致地址冲突，有些操作系统会出现网络失效。通过广播分发请求，同样需要修改服务器操作系统的源码来过滤报文，使得只 有一台服务器处理广播来的请求。 微软的Windows NT负载均衡服务（Windows NT Load Balancing Service，WLBS）[12]是1998年底收购Valence Research公司获得的，它与ONE-IP中的基于本地过滤方法一样。WLBS作为过滤器运行在网卡驱动程序和TCP/IP协议栈之间，获得目标地址 为VIP的报文，它的过滤算法检查报文的源IP地址和端口号，保证只有一台服务器将报文交给上一层处理。但是，当有新结点加入和有结点失效时，所有服务器 需要协商一个新的过滤算法，这会导致所有有Session的连接中断。同时，WLBS需要所有的服务器有相同的配置，如网卡速度和处理能力。 3 通过NAT实现虚拟服务器（VS/NAT）由于IPv4中IP地址空间的日益紧张和安全方面的原因，很多网络使用保留IP地址（10.0.0.0/255.0.0.0、 172.16.0.0/255.128.0.0和192.168.0.0/255.255.0.0）[64, 65, 66]。这些地址不在Internet上使用，而是专门为内部网络预留的。当内部网络中的主机要访问Internet或被Internet访问时，就需要 采用网络地址转换（Network Address Translation, 以下简称NAT），将内部地址转化为Internets上可用的外部地址。NAT的工作原理是报文头（目标地址、源地址和端口等）被正确改写后，客户相信 它们连接一个IP地址，而不同IP地址的服务器组也认为它们是与客户直接相连的。由此，可以用NAT方法将不同IP地址的并行网络服务变成在一个IP地址 上的一个虚拟服务。 VS/NAT的体系结构如图2所示。在一组服务器前有一个调度器，它们是通过Switch/HUB相连接的。这些服务器 提供相同的网络服务、相同的内容，即不管请求被发送到哪一台服务器，执行结果是一样的。服务的内容可以复制到每台服务器的本地硬盘上，可以通过网络文件系 统（如NFS）共享，也可以通过一个分布式文件系统来提供。 客户通过Virtual IP Address（虚拟服务的IP地址）访问网络服务时，请求报文到达调度器，调度器根据连接调度算法从一组真实服务器中选出一台服务器，将报文的目标地址 Virtual IP Address改写成选定服务器的地址，报文的目标端口改写成选定服务器的相应端口，最后将修改后的报文发送给选出的服务器。同时，调度器在连接Hash 表中记录这个连接，当这个连接的下一个报文到达时，从连接Hash表中可以得到原选定服务器的地址和端口，进行同样的改写操作，并将报文传给原选定的服务 器。当来自真实服务器的响应报文经过调度器时，调度器将报文的源地址和源端口改为Virtual IP Address和相应的端口，再把报文发给用户。我们在连接上引入一个状态机，不同的报文会使得连接处于不同的状态，不同的状态有不同的超时值。在TCP 连接中，根据标准的TCP有限状态机进行状态迁移，这里我们不一一叙述，请参见W. Richard Stevens的《TCP/IP Illustrated Volume I》；在UDP中，我们只设置一个UDP状态。不同状态的超时值是可以设置的，在缺省情况下，SYN状态的超时为1分钟，ESTABLISHED状态的超 时为15分钟，FIN状态的超时为1分钟；UDP状态的超时为5分钟。当连接终止或超时，调度器将这个连接从连接Hash表中删除。 这样，客户所看到的只是在Virtual IP Address上提供的服务，而服务器集群的结构对用户是透明的。对改写后的报文，应用增量调整Checksum的算法调整TCP Checksum的值，避免了扫描整个报文来计算Checksum的开销。 在 一些网络服务中，它们将IP地址或者端口号在报文的数据中传送，若我们只对报文头的IP地址和端口号作转换，这样就会出现不一致性，服务会中断。所以，针 对这些服务，需要编写相应的应用模块来转换报文数据中的IP地址或者端口号。我们所知道有这个问题的网络服务有FTP、IRC、H.323、 CUSeeMe、Real Audio、Real Video、Vxtreme / Vosiac、VDOLive、VIVOActive、True Speech、RSTP、PPTP、StreamWorks、NTT AudioLink、NTT SoftwareVision、Yamaha MIDPlug、iChat Pager、Quake和Diablo。 下面，举个例子来进一步说明VS/NAT，如图3所示： VS/NAT的配置如下表所示，所有到IP地址为202.103.106.5和端口为80的流量都被负载均衡地调度的真实服务器172.16.0.2:80和 172.16.0.3:8000上。目标地址为202.103.106.5:21的报文被转移到172.16.0.3:21上。而到其他端口的报文将被拒绝。 Protocol Virtual IP Address Port Real IP Address Port Weight TCP 202.103.106.5 80 172.16.0.2 80 1 172.16.0.3 80 2 TCP 202.103.106.5 21 172.16.0.3 21 1 从以下的例子中，我们可以更详细地了解报文改写的流程。访问Web服务的报文可能有以下的源地址和目标地址： SOURCE 202.100.1.2:3456 DEST 202.103.106.5:80 调度器从调度列表中选出一台服务器，例如是172.16.0.3:8000。该报文会被改写为如下地址，并将它发送给选出的服务器。 SOURCE 202.100.1.2:3456 DEST 172.16.0.3:8000 从服务器返回到调度器的响应报文如下： SOURCE 172.16.0.3:8000 DEST 202.100.1.2:3456 响应报文的源地址会被改写为虚拟服务的地址，再将报文发送给客户： SOURCE 202.103.106.5:80 DEST 202.100.1.2:3456 这样，客户认为是从202.103.106.5:80服务得到正确的响应，而不会知道该请求是服务器172.16.0.2还是服务器172.16.0.3处理的。 4 通过IP隧道实现虚拟服务器（VS/TUN）在VS/NAT 的集群系统中，请求和响应的数据报文都需要通过负载调度器，当真实服务器的数目在10台和20台之间时，负载调度器将成为整个集群系统的新瓶颈。大多数 Internet服务都有这样的特点：请求报文较短而响应报文往往包含大量的数据。如果能将请求和响应分开处理，即在负载调度器中只负责调度请求而响应直 接返回给客户，将极大地提高整个集群系统的吞吐量。 IP隧道（IP tunneling）是将一个IP报文封装在另一个IP报文的技术，这可以使得目标为一个IP地址的数据报文能被封装和转发到另一个IP地址。IP隧道技 术亦称为IP封装技术（IP encapsulation）。IP隧道主要用于移动主机和虚拟私有网络（Virtual Private Network），在其中隧道都是静态建立的，隧道一端有一个IP地址，另一端也有唯一的IP地址。 我们利用IP隧道技术将请求报文封装转 发给后端服务器，响应报文能从后端服务器直接返回给客户。但在这里，后端服务器有一组而非一个，所以我们不可能静态地建立一一对应的隧道，而是动态地选择 一台服务器，将请求报文封装和转发给选出的服务器。这样，我们可以利用IP隧道的原理将一组服务器上的网络服务组成在一个IP地址上的虚拟网络服务。 VS/TUN的体系结构如图4所示，各个服务器将VIP地址配置在自己的IP隧道设备上。 VS/TUN 的工作流程如图5所示：它的连接调度和管理与VS/NAT中的一样，只是它的报文转发方法不同。调度器根据各个服务器的负载情况，动态地选择一台服务器， 将请求报文封装在另一个IP报文中，再将封装后的IP报文转发给选出的服务器；服务器收到报文后，先将报文解封获得原来目标地址为VIP的报文，服务器发 现VIP地址被配置在本地的IP隧道设备上，所以就处理这个请求，然后根据路由表将响应报文直接返回给客户。在这里需要指出，根据缺省的TCP/IP协议栈处理，请求报文的目标地址为VIP，响应报文的源地址肯定也为VIP，所以响应报文不需要作任何修改，可以直接返回给客户，客户认为得到正常的服务，而不会知道究竟是哪一台服务器处理的。 5 通过直接路由实现虚拟服务器（VS/DR）跟VS/TUN 方法相同，VS/DR利用大多数Internet服务的非对称特点，负载调度器中只负责调度请求，而服务器直接将响应返回给客户，可以极大地提高整个集群 系统的吞吐量。该方法与IBM的NetDispatcher产品中使用的方法类似（其中服务器上的IP地址配置方法是相似的），但IBM的 NetDispatcher是非常昂贵的商品化产品，我们也不知道它内部所使用的机制，其中有些是IBM的专利。 VS/DR的体系结构如图 7所示：调度器和服务器组都必须在物理上有一个网卡通过不分断的局域网相连，如通过高速的交换机或者HUB相连。VIP地址为调度器和服务器组共享，调度 器配置的VIP地址是对外可见的，用于接收虚拟服务的请求报文；所有的服务器把VIP地址配置在各自的Non-ARP网络设备上，它对外面是不可见的，只 是用于处理目标地址为VIP的网络请求。VS/DR 的工作流程如图8所示：它的连接调度和管理与VS/NAT和VS/TUN中的一样，它的报文转发方法又有不同，将报文直接路由给目标服务器。在VS/DR 中，调度器根据各个服务器的负载情况，动态地选择一台服务器，不修改也不封装IP报文，而是将数据帧的MAC地址改为选出服务器的MAC地址，再将修改后 的数据帧在与服务器组的局域网上发送。因为数据帧的MAC地址是选出的服务器，所以服务器肯定可以收到这个数据帧，从中可以获得该IP报文。当服务器发现 报文的目标地址VIP是在本地的网络设备上，服务器处理这个报文，然后根据路由表将响应报文直接返回给客户。在VS/DR中，根据缺省的TCP/IP协议栈处理，请求报文的目标地址为VIP，响应报文的源地址肯定也为VIP，所以响应报文不需要作任何修改，可以直接返回给客户，客户认为得到正常的服务，而不会知道是哪一台服务器处理的。VS/DR负载调度器跟VS/TUN一样只处于从客户到服务器的半连接中，按照半连接的TCP有限状态机进行状态迁移。 6 三种方法的优缺点比较三种IP负载均衡技术的优缺点归纳在下表中： _ VS/NAT VS/TUN VS/DR Server any Tunneling Non-arp device server network private LAN/WAN LAN server number low (10~20) High (100) High (100) server gateway load balancer own router Own router 注： 以上三种方法所能支持最大服务器数目的估计是假设调度器使用100M网卡，调度器的硬件配置与后端服务器的硬件配置相同，而且是对一般Web服务。使用更高的硬件配置（如千兆网卡和更快的处理器）作为调度器，调度器所能调度的服务器数量会相应增加。当应用不同时，服务器的数目也会相应地改变。所以，以上数 据估计主要是为三种方法的伸缩性进行量化比较。 6.1 Virtual Server via NATVS/NAT 的优点是服务器可以运行任何支持TCP/IP的操作系统，它只需要一个IP地址配置在调度器上，服务器组可以用私有的IP地址。缺点是它的伸缩能力有限， 当服务器结点数目升到20时，调度器本身有可能成为系统的新瓶颈，因为在VS/NAT中请求和响应报文都需要通过负载调度器。 我们在Pentium 166 处理器的主机上测得重写报文的平均延时为60us，性能更高的处理器上延时会短一些。假设TCP报文的平均长度为536 Bytes，则调度器的最大吞吐量为8.93 MBytes/s. 我们再假设每台服务器的吞吐量为800KBytes/s，这样一个调度器可以带动10台服务器。（注：这是很早以前测得的数据） 基于 VS/NAT的的集群系统可以适合许多服务器的性能要求。如果负载调度器成为系统新的瓶颈，可以有三种方法解决这个问题：混合方法、VS/TUN和 VS/DR。在DNS混合集群系统中，有若干个VS/NAT负载调度器，每个负载调度器带自己的服务器集群，同时这些负载调度器又通过RR-DNS组成简 单的域名。但VS/TUN和VS/DR是提高系统吞吐量的更好方法。 对于那些将IP地址或者端口号在报文数据中传送的网络服务，需要编写相应的应用模块来转换报文数据中的IP地址或者端口号。这会带来实现的工作量，同时应用模块检查报文的开销会降低系统的吞吐率。 6.2 Virtual Server via IP Tunneling在VS/TUN 的集群系统中，负载调度器只将请求调度到不同的后端服务器，后端服务器将应答的数据直接返回给用户。这样，负载调度器就可以处理大量的请求，它甚至可以调 度百台以上的服务器（同等规模的服务器），而它不会成为系统的瓶颈。即使负载调度器只有100Mbps的全双工网卡，整个系统的最大吞吐量可超过 1Gbps。所以，VS/TUN可以极大地增加负载调度器调度的服务器数量。VS/TUN调度器可以调度上百台服务器，而它本身不会成为系统的瓶颈，可以 用来构建高性能的超级服务器。 VS/TUN技术对服务器有要求，即所有的服务器必须支持“IP Tunneling”或者“IP Encapsulation”协议。目前，VS/TUN的后端服务器主要运行Linux操作系统，我们没对其他操作系统进行测试。因为“IP Tunneling”正成为各个操作系统的标准协议，所以VS/TUN应该会适用运行其他操作系统的后端服务器。 6.3 Virtual Server via Direct Routing跟VS/TUN方法一样，VS/DR调度器只处理客户到服务器端的连接，响应数据可以直接从独立的网络路由返回给客户。这可以极大地提高LVS集群系统的伸缩性。 跟VS/TUN相比，这种方法没有IP隧道的开销，但是要求负载调度器与实际服务器都有一块网卡连在同一物理网段上，服务器网络设备（或者设备别名）不作ARP响应，或者能将报文重定向（Redirect）到本地的Socket端口上。 7 小结本文主要讲述了LVS集群中的三种IP负载均衡技术。在分析网络地址转换方法（VS/NAT）的缺点和网络服务的非对称性的基础上，我们给出了通过IP隧道实现虚拟服务器的方法VS/TUN，和通过直接路由实现虚拟服务器的方法VS/DR，极大地提高了系统的伸缩性。 参考文献 Eric Dean Katz, Michelle Butler, and Robert McGrath, “A Scalable HTTP Server: The NCSA Prototype”, Computer Networks and ISDN Systems, pp155-163, 1994. Thomas T. Kwan, Robert E. McGrath, and Daniel A. Reed, “NCSA’s World Wide Web Server: Design and Performance”, IEEE Computer, pp68-74, November 1995. C. Yoshikawa, B. Chun, P. Eastham, A. Vahdat, T. Anderson, and D. Culler. Using Smart Clients to Build Scalable Services. In Proceedings of the 1997 USENIX Technical Conference, January 1997. Zeus Technology, Inc. Zeus Load Balancer v1.1 User Guide. http://www.zeus.com/ Edward Walker, “pWEB - A Parallel Web Server Harness”, http://www.ihpc.nus.edu.sg/STAFF/edward/pweb.html, April, 1997. Ralf S.Engelschall. Load Balancing Your Web Site: Practical Approaches for Distributing HTTP Traffic. Web Techniques Magazine, Volume 3, Issue 5, http://www.webtechniques.com, May 1998. Daniel Andresen, Tao Yang, Oscar H. Ibarra. Towards a Scalable Distributed WWW Server on Workstation Clusters. In Proceedings of 10th IEEE International Symposium Of Parallel Processing (IPPS’96), pp.850-856, http://www.cs.ucsb.edu-/Research/rapid_sweb/SWEB.html, April 1996. Eric Anderson, Dave Patterson, and Eric Brewer, “The Magicrouter: an Application of Fast Packet Interposing”, http://www.cs.berkeley.edu/~eanders-/magicrouter/, May, 1996. D. Dias, W. Kish, R. Mukherjee, and R. Tewari. A Scalable and Highly Available Server. In Proceeding of COMPCON 1996, IEEE-CS Press, Santa Clara, CA, USA, Febuary 1996, pp. 85-92. Guerney D.H. Hunt, German S. Goldszmidt, Richard P. King, and Rajat Mukherjee. Network Dispatcher: a connection router for scalable Internet services. In Proceedings of the 7th International WWW Conference, Brisbane, Australia, April 1998. Om P. Damani, P. Emerald Chung, Yennun Huang, “ONE-IP: Techniques for Hosting a Service on a Cluster of Machines”, http://www.cs.utexas.edu-/users/damani/, August 1997. Microsoft Corporation. Microsoft Windows NT Load Balancing Service. http://www.micorsoft.com/ntserver/NTServerEnterprise/exec/feature/WLBS/. 关于作者章文嵩博士，开放源码及Linux内核的开发者，著名的Linux集群项目－－LVS(Linux Virtual Server)的创始人和主要开发人员。他目前工作于国家并行与分布式处理重点实验室，主要从事集群技术、操作系统、对象存储与数据库的研究。他一直在自 由软件的开发上花费大量时间，并以此为乐。","categories":[{"name":"Network","slug":"Network","permalink":"https://delacroix.github.io/categories/Network/"}],"tags":[{"name":"负载均衡","slug":"负载均衡","permalink":"https://delacroix.github.io/tags/负载均衡/"}]}]}